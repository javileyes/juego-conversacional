{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92c26d9",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/semantic-router/blob/main/docs/05-local-execution.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/aurelio-labs/semantic-router/blob/main/docs/05-local-execution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50410e-3f98-4d9c-8838-b38aebd6ce77",
   "metadata": {},
   "source": [
    "# Local Dynamic Routes\n",
    "\n",
    "## Fully local Semantic Router with `llama.cpp` and HuggingFace Encoder\n",
    "\n",
    "There are many reasons users might choose to roll their own LLMs rather than use a third-party service. Whether it's due to cost, privacy or compliance, Semantic Router supports the use of \"local\" LLMs through `llama.cpp`.\n",
    "\n",
    "Using `llama.cpp` also enables the use of quantized GGUF models, reducing the memory footprint of deployed models, allowing even 13-billion parameter models to run with hardware acceleration on an Apple M1 Pro chip.\n",
    "\n",
    "Below is an example of using semantic router with **Mistral-7B-Instruct**, quantized i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8d577-9f23-4dec-b167-fdecfb313c52",
   "metadata": {},
   "source": [
    "## Installing the library\n",
    "\n",
    "> Note: if you require hardware acceleration via BLAS, CUDA, Metal, etc. please refer to the [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python#installation-with-specific-hardware-acceleration-blas-cuda-metal-etc) repository README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95e4906-c3e6-4905-8f13-5e67d67069d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"semantic-router[local]==0.0.17\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029cc6d",
   "metadata": {},
   "source": [
    "(NO ES EL CASO) If you're running on Apple silicon you can run the following to run with Metal hardware acceleration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9b5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d05a350",
   "metadata": {},
   "source": [
    "PARA INSTALAR LLAMA-CPP-PYTHON CON SOPORTE GPU (CUDA) EJECUTAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76460e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:17:24_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c3c2e8",
   "metadata": {},
   "source": [
    "Esto me carga el modelo en GPU pero luego me crashea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6b855d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3.1 from /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.31.tar.gz (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting scikit-build-core>=0.5.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached scikit_build_core-0.7.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Collecting exceptiongroup (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Collecting packaging>=20.9 (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Collecting tomli>=1.1 (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Using cached scikit_build_core-0.7.1-py3-none-any.whl (136 kB)\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
      "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.7.1 tomli-2.0.1\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l  Running command pip subprocess to install backend dependencies\n",
      "  Collecting cmake>=3.21\n",
      "    Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "  Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "  Installing collected packages: cmake\n",
      "  Successfully installed cmake-3.28.1\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.7.1\u001b[0m using \u001b[94mCMake 3.28.1\u001b[0m \u001b[91m(metadata_wheel)\u001b[0m\u001b[0m\n",
      "\u001b[?25hdone\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/a5/37/d1453c9ff4f7630e68ec036c6fb56ba0d7c769daa8a4083cb4ef8ee45995/numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m668.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/30/6d/6de6be2d02603ab56e72997708809e8a5b0fbfee080735109b40a3564843/Jinja2-3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/36/2a/fab302636634e1f770a26aac212e44cff25522ed3c9189bd8afc9ae2effd/MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m645.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m772.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.7.1\u001b[0m using \u001b[94mCMake 3.28.1\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2024-01-22 02:05:04,347 - scikit_build_core - WARNING - libdir/ldlibrary: /home/javier/miniconda3/envs/llama-cpp/lib/libpython3.10.a is not a real file!\n",
      "  2024-01-22 02:05:04,347 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/home/javier/miniconda3/envs/llama-cpp/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\n",
      "  loading initial cache file /tmp/tmpcf81usq3/build/CMakeInit.txt\n",
      "  -- The C compiler identification is GNU 11.3.0\n",
      "  -- The CXX compiler identification is GNU 11.3.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found CUDAToolkit: /usr/local/cuda-12.3/targets/x86_64-linux/include (found version \"12.3.52\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is NVIDIA 12.3.52\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/local/cuda-12.3/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- Using CUDA architectures: 52;61;70\n",
      "  \u001b[0m-- CUDA host compiler is GNU 11.3.0\n",
      "  \u001b[0m\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:30 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  -- Configuring done (1.2s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/tmpcf81usq3/build\n",
      "  \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  Change Dir: '/tmp/tmpcf81usq3/build'\n",
      "\n",
      "  Run Build Command(s): /home/javier/miniconda3/envs/llama-cpp/bin/ninja -v\n",
      "  [1/23] cd /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp && /tmp/pip-build-env-dp2ubb6t/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.3.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  [2/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/build-info.cpp\n",
      "  [3/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/console.cpp\n",
      "  [4/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-alloc.c\n",
      "  [5/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/llava.cpp\n",
      "  [6/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-backend.c\n",
      "  [7/23] /usr/bin/c++  -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
      "  [8/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/grammar-parser.cpp\n",
      "  [9/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/sampling.cpp\n",
      "  [10/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/train.cpp\n",
      "  [11/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-quants.c\n",
      "  [12/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/common/common.cpp\n",
      "  [13/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml.c\n",
      "  [14/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/examples/llava/clip.cpp\n",
      "  [15/23] : && /tmp/pip-build-env-dp2ubb6t/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
      "  [16/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/llama.cpp\n",
      "  [17/23] /usr/local/cuda-12.3/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -use_fast_math -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
      "  /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-cuda.cu(609): warning #177-D: function \"warp_reduce_sum(half2)\" was declared but never referenced\n",
      "                                     half2 warp_reduce_sum(half2 a) {\n",
      "                                           ^\n",
      "\n",
      "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "  /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/vendor/llama.cpp/ggml-cuda.cu(630): warning #177-D: function \"warp_reduce_max(half2)\" was declared but never referenced\n",
      "                                     half2 warp_reduce_max(half2 x) {\n",
      "                                           ^\n",
      "\n",
      "  [18/23] : && /tmp/pip-build-env-dp2ubb6t/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
      "  [19/23] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda-12.3/targets/x86_64-linux/lib\" && :\n",
      "  [20/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs   -L/usr/local/cuda-12.3/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.3/targets/x86_64-linux/lib:  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
      "  [21/23] : && /tmp/pip-build-env-dp2ubb6t/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
      "  [22/23] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmpcf81usq3/build/vendor/llama.cpp:/usr/local/cuda-12.3/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "  [23/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  -Wl,-rpath,/tmp/tmpcf81usq3/build/vendor/llama.cpp:/usr/local/cuda-12.3/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "\n",
      "  \u001b[92m***\u001b[0m \u001b[1mInstalling project into wheel...\u001b[0m\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/include/ggml.h\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/include/ggml-alloc.h\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/include/ggml-backend.h\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/include/ggml-cuda.h\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/lib/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpcf81usq3/wheel/platlib/lib/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/include/llama.h\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/bin/convert.py\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/llama_cpp/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpcf81usq3/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/llama_cpp/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/llama_cpp/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/lib/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpcf81usq3/wheel/platlib/lib/libllava.so\" to \"\"\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/bin/llava-cli\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpcf81usq3/wheel/platlib/bin/llava-cli\" to \"\"\n",
      "  -- Installing: /tmp/tmpcf81usq3/wheel/platlib/llama_cpp/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmpcf81usq3/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/llama_cpp/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-t2q09sm7/llama-cpp-python_242256e3249244cca4a3623eeb590bb1/llama_cpp/libllava.so\" to \"\"\n",
      "  \u001b[92m***\u001b[0m \u001b[1mMaking wheel...\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mCreated\u001b[22m llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl...\u001b[0m\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl size=9631888 sha256=7eb96116da14b399b4bbd02622c258b261d114d4fa8bdbf5cc9810125a8fa8c6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fs129v8c/wheels/26/8c/d2/2432edacf8fe69a190cc2a462fa5aea8eaf13f79c1efdbf6d7\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/typing_extensions-4.9.0.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/bin/f2py\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/numpy-1.26.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/numpy/\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  changing mode of /home/javier/miniconda3/envs/llama-cpp/bin/f2py to 775\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/MarkupSafe-2.1.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/markupsafe/\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/diskcache-5.6.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/Jinja2-3.1.2.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/jinja2/\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.31\n",
      "    Uninstalling llama_cpp_python-0.2.31:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/bin/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/include/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/lib/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/llama_cpp/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/llama_cpp_python-0.2.31.dist-info/\n",
      "      Successfully uninstalled llama_cpp_python-0.2.31\n",
      "Successfully installed MarkupSafe-2.1.4 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.31 numpy-1.26.3 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7c6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.2.31\n",
      "Uninstalling llama_cpp_python-0.2.31:\n",
      "  Successfully uninstalled llama_cpp_python-0.2.31\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd80747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
      "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: llama-cpp-python\n",
      "Successfully installed llama-cpp-python-0.2.31\n"
     ]
    }
   ],
   "source": [
    "#With NVidia CUDA acceleration\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"  pip install llama-cpp-python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae1148",
   "metadata": {},
   "source": [
    "## Como tengo problemas de incompatibilidad de xformers con torch (debería ser 2.1.0) en lugar de arriesgarme a cambiarlo que y se me rompan otras cosas voy a crear un entorno nuevo con conda y voy a instalar todo desde cero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e38e4",
   "metadata": {},
   "source": [
    "```bash\n",
    "conda create -n llama-cpp python=3.10\n",
    "conda activate llama-cpp\n",
    "conda install pytorch==2.1.0 -c pytorch\n",
    "conda install pytorch==2.1.0 torchvision torchaudio cudatoolkit=12.2 -c pytorch #la versión de CUDA del sistema es 12.2 (se ve con nvidia-smi)\n",
    "\n",
    "#verificar\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "#2.1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0f5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'llama.cpp'...\n",
      "Updating files: 100% (398/398), done.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
      "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
      "I NVCCFLAGS: -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
      "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "I CC:        cc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\n",
      "I CXX:       g++ (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\n",
      "\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "common/common.cpp: In function ‘bool gpt_params_parse_ex(int, char**, gpt_params&)’:\n",
      "common/common.cpp:223:23: warning: cast from type ‘const char*’ to type ‘char*’ casts away qualifiers [-Wcast-qual]\n",
      "  223 |             file.read((char *)params.prompt.data(), size);\n",
      "      |                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
      "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
      "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "\n",
      "====  Run ./main -h for help.  ====\n",
      "\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "examples/perplexity/perplexity.cpp: In function ‘bool deserialize_string(std::istream&, std::string&)’:\n",
      "examples/perplexity/perplexity.cpp:1038:22: warning: cast from type ‘const char*’ to type ‘char*’ casts away qualifiers [-Wcast-qual]\n",
      " 1038 |         if (!in.read((char *)str.data(), size).fail()) return true;\n",
      "      |                      ^~~~~~~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/imatrix/imatrix.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o imatrix -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib   -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib  -Wno-cast-qual\n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
      "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd llama.cpp\n",
    "make LLAMA_CUBLAS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b68eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: llama_cpp_python 0.2.31\n",
      "Uninstalling llama_cpp_python-0.2.31:\n",
      "  Successfully uninstalled llama_cpp_python-0.2.31\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b569b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_HOME=/usr/local/cuda-12/\n"
     ]
    }
   ],
   "source": [
    "# poner variables de entorno en cuaderno jupyter\n",
    "%env CUDA_HOME=/usr/local/cuda-12/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d118806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 23.3.1 from /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.31.tar.gz (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l  Running command pip subprocess to install build dependencies\n",
      "  Collecting scikit-build-core>=0.5.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached scikit_build_core-0.7.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Collecting exceptiongroup (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Collecting packaging>=20.9 (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "  Collecting tomli>=1.1 (from scikit-build-core>=0.5.1->scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n",
      "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
      "  Using cached scikit_build_core-0.7.1-py3-none-any.whl (136 kB)\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "  Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
      "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.12.1 pyproject-metadata-0.7.1 scikit-build-core-0.7.1 tomli-2.0.1\n",
      "\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l  Running command Getting requirements to build wheel\n",
      "\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l  Running command pip subprocess to install backend dependencies\n",
      "  Collecting cmake>=3.21\n",
      "    Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "  Using cached cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "  Installing collected packages: cmake\n",
      "  Successfully installed cmake-3.28.1\n",
      "\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l  Running command Preparing metadata (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.7.1\u001b[0m using \u001b[94mCMake 3.28.1\u001b[0m \u001b[91m(metadata_wheel)\u001b[0m\u001b[0m\n",
      "\u001b[?25hdone\n",
      "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/a5/37/d1453c9ff4f7630e68ec036c6fb56ba0d7c769daa8a4083cb4ef8ee45995/numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m555.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/30/6d/6de6be2d02603ab56e72997708809e8a5b0fbfee080735109b40a3564843/Jinja2-3.1.3-py3-none-any.whl.metadata\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/36/2a/fab302636634e1f770a26aac212e44cff25522ed3c9189bd8afc9ae2effd/MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m587.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m816.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "  \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.7.1\u001b[0m using \u001b[94mCMake 3.28.1\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  2024-01-22 02:35:31,182 - scikit_build_core - WARNING - libdir/ldlibrary: /home/javier/miniconda3/envs/llama-cpp/lib/libpython3.10.a is not a real file!\n",
      "  2024-01-22 02:35:31,182 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/home/javier/miniconda3/envs/llama-cpp/lib, ldlibrary=libpython3.10.a, multiarch=x86_64-linux-gnu, masd=None\n",
      "  loading initial cache file /tmp/tmp8ruzehdm/build/CMakeInit.txt\n",
      "  -- The C compiler identification is GNU 11.3.0\n",
      "  -- The CXX compiler identification is GNU 11.3.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Found CUDAToolkit: /usr/local/cuda-12.3/targets/x86_64-linux/include (found version \"12.3.52\")\n",
      "  -- cuBLAS found\n",
      "  -- The CUDA compiler identification is NVIDIA 12.3.52\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/local/cuda-12/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- Using CUDA architectures: 52;61;70\n",
      "  \u001b[0m-- CUDA host compiler is GNU 11.3.0\n",
      "  \u001b[0m\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- x86 detected\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  \u001b[33mCMake Warning (dev) at CMakeLists.txt:30 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "  \u001b[0m\n",
      "  -- Configuring done (1.1s)\n",
      "  -- Generating done (0.0s)\n",
      "  -- Build files have been written to: /tmp/tmp8ruzehdm/build\n",
      "  \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  Change Dir: '/tmp/tmp8ruzehdm/build'\n",
      "\n",
      "  Run Build Command(s): /home/javier/miniconda3/envs/llama-cpp/bin/ninja -v\n",
      "  [1/23] cd /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp && /tmp/pip-build-env-g_v_jwgu/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.3.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
      "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "  [2/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/build-info.cpp\n",
      "  [3/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/console.cpp\n",
      "  [4/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-alloc.c\n",
      "  [5/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/llava.cpp\n",
      "  [6/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-backend.c\n",
      "  [7/23] /usr/bin/c++  -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
      "  [8/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/grammar-parser.cpp\n",
      "  [9/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/sampling.cpp\n",
      "  [10/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/train.cpp\n",
      "  [11/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-quants.c\n",
      "  [12/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/common/common.cpp\n",
      "  [13/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml.c\n",
      "  [14/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/examples/llava/clip.cpp\n",
      "  [15/23] : && /tmp/pip-build-env-g_v_jwgu/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
      "  [16/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/llama.cpp\n",
      "  [17/23] /usr/local/cuda-12/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/. -isystem /usr/local/cuda-12.3/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -use_fast_math -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
      "  /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-cuda.cu(609): warning #177-D: function \"warp_reduce_sum(half2)\" was declared but never referenced\n",
      "                                     half2 warp_reduce_sum(half2 a) {\n",
      "                                           ^\n",
      "\n",
      "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
      "\n",
      "  /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/vendor/llama.cpp/ggml-cuda.cu(630): warning #177-D: function \"warp_reduce_max(half2)\" was declared but never referenced\n",
      "                                     half2 warp_reduce_max(half2 x) {\n",
      "                                           ^\n",
      "\n",
      "  [18/23] : && /tmp/pip-build-env-g_v_jwgu/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
      "  [19/23] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda-12/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda-12/targets/x86_64-linux/lib\" && :\n",
      "  [20/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda-12/targets/x86_64-linux/lib/stubs   -L/usr/local/cuda-12/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-12.3/targets/x86_64-linux/lib:  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
      "  [21/23] : && /tmp/pip-build-env-g_v_jwgu/normal/lib/python3.10/site-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
      "  [22/23] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmp8ruzehdm/build/vendor/llama.cpp:/usr/local/cuda-12.3/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "  [23/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  -Wl,-rpath,/tmp/tmp8ruzehdm/build/vendor/llama.cpp:/usr/local/cuda-12.3/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-12.3/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-12.3/targets/x86_64-linux/lib/stubs/libcuda.so && :\n",
      "\n",
      "  \u001b[92m***\u001b[0m \u001b[1mInstalling project into wheel...\u001b[0m\n",
      "  -- Install configuration: \"Release\"\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/lib/libggml_shared.so\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/include/ggml.h\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/include/ggml-alloc.h\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/include/ggml-backend.h\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/include/ggml-cuda.h\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/lib/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp8ruzehdm/wheel/platlib/lib/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/include/llama.h\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/bin/convert.py\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/bin/convert-lora-to-ggml.py\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/llama_cpp/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp8ruzehdm/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/llama_cpp/libllama.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/llama_cpp/libllama.so\" to \"\"\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/lib/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp8ruzehdm/wheel/platlib/lib/libllava.so\" to \"\"\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/bin/llava-cli\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp8ruzehdm/wheel/platlib/bin/llava-cli\" to \"\"\n",
      "  -- Installing: /tmp/tmp8ruzehdm/wheel/platlib/llama_cpp/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/tmp8ruzehdm/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
      "  -- Installing: /tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/llama_cpp/libllava.so\n",
      "  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-8pi2uqkl/llama-cpp-python_5daa44256c11451584a1f66ab9cccdec/llama_cpp/libllava.so\" to \"\"\n",
      "  \u001b[92m***\u001b[0m \u001b[1mMaking wheel...\u001b[0m\n",
      "  \u001b[92m***\u001b[0m \u001b[1mCreated\u001b[22m llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl...\u001b[0m\n",
      "\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl size=9631798 sha256=3f862802f831d7184a19ca1f775b43835f3544266fad22ab647ca95192d7761f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ixtp0tzm/wheels/26/8c/d2/2432edacf8fe69a190cc2a462fa5aea8eaf13f79c1efdbf6d7\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/typing_extensions-4.9.0.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/typing_extensions.py\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/bin/f2py\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/numpy-1.26.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/numpy.libs/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/numpy/\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "  changing mode of /home/javier/miniconda3/envs/llama-cpp/bin/f2py to 775\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.4\n",
      "    Uninstalling MarkupSafe-2.1.4:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/MarkupSafe-2.1.4.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/markupsafe/\n",
      "      Successfully uninstalled MarkupSafe-2.1.4\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/diskcache-5.6.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/diskcache/\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/Jinja2-3.1.3.dist-info/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/jinja2/\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.31\n",
      "    Uninstalling llama_cpp_python-0.2.31:\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/bin/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/include/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/lib/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/llama_cpp/\n",
      "      Removing file or directory /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages/llama_cpp_python-0.2.31.dist-info/\n",
      "      Successfully uninstalled llama_cpp_python-0.2.31\n",
      "Successfully installed MarkupSafe-2.1.4 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.31 numpy-1.26.3 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_COMPILER=/usr/local/cuda-12/bin/nvcc \" FORCE_CMAKE=1 pip install --break-system-packages  --upgrade --force-reinstall llama-cpp-python --no-cache-dir --verbose;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163ac7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.2.31-cp310-cp310-manylinux_2_35_x86_64.whl\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages (from llama-cpp-python) (1.26.3)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/javier/miniconda3/envs/llama-cpp/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n",
      "Installing collected packages: llama-cpp-python\n",
      "Successfully installed llama-cpp-python-0.2.31\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\"  pip install llama-cpp-python \n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f52f11-ae6d-4706-8da3-ce03a7a6b92d",
   "metadata": {},
   "source": [
    "## Download the Mistral 7B Instruct 4-bit GGUF files\n",
    "\n",
    "We will be using Mistral 7B Instruct, quantized as a 4-bit GGUF file, a good balance between performance and ability to deploy on consumer hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6ddf61-c189-4b3b-99df-9508f830ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1176  100  1176    0     0   2838      0 --:--:-- --:--:-- --:--:--  2840\n",
      "100 4893M  100 4893M    0     0  37.8M      0  0:02:09  0:02:09 --:--:-- 37.7M13  0:00:57 39.1M\n",
      "mistral-7b-instruct-v0.2.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "! curl -L \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf?download=true\" -o ./mistral-7b-instruct-v0.2.Q5_K_M.gguf\n",
    "! ls mistral-7b-instruct-v0.2.Q5_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dd1a45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   156.00 MiB\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "enable_gpu = True  # offload LLM layers to the GPU (must fit in memory)\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"./mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=-1 if enable_gpu else 0,\n",
    "    n_ctx=2048,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca7aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-716b586d-2019-4464-af7c-3f4bf67861ca',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1705912172,\n",
       " 'model': './mistral-7b-instruct-v0.2.Q5_K_M.gguf',\n",
       " 'choices': [{'text': '<s>[INST] ¿Puedes escribirme una SPARQL query completa (usando Select, filter, service) para obtener el nombre de los países que tienen una población mayor a 100 millones de habitantes? [/INST] I\\'m assuming you have a SPARQL-enabled dataset about countries and their populations available through a SPARQL endpoint. Here is an example of a query using DBpedia as the data source:\\n\\n```sparql\\nPREFIX dbo: <http://dbpedia.org/ontology/>\\nSELECT ?countryName WHERE {\\n  SERVICE <https://query.wikidata.org/sparql> {\\n    SELECT DISTINCT ?countryWikidataId ?population (STR(?population) AS ?populationStr) WHERE {\\n      ?country wdt:P31 dbo:Country .\\n      ?country wdt:P627-1 ?capital.\\n      OPTIONAL { ?country wdt:P1820 ?population }.\\n      FILTER(LANG(?capital) = \"en\" && STRSTARTS(STR(?population), \">\") && REGEX(STR(?populationStr), \"\\\\d{3,}\"))\\n      FILTER(strlen(strReplace(strReplace(str(?populationStr), \"<[^>]*>\", \"\", \"g\"), \"[^\\\\d]\", \"\")) > 9)\\n    }\\n    FILTER(STRTOINT(substr(str(?populationStr), INSTR(str(?populationStr), \\'>\\')+1)) > 100000000)\\n    SERVICE <http://dbpedia.org/sparql> {\\n      BIND(IRI(\"http://dbpedia.org/resource/\" + REPLACE(STR(?countryWikidataId), \"/[^/]+$\", \"\") as ?countryURI) )\\n      SELECT ?countryName WHERE {\\n        ?country dbo:capital ?capital .\\n        SERVICE dbpedia:property <http://dbpedia.org/ontology/name> {\\n          BIND(?value AS ?countryName)\\n        }\\n      }\\n    }\\n  }\\n}\\nORDER BY DESC(STRTOINT(substr(str(?populationStr), INSTR(str(?populationStr), \\'>\\')+1)))\\n```\\n\\nThis query uses two `SERVICE` statements to access both DBpedia and Wikidata. It first searches for countries with their respective capitals, population data from Wikidata, and filters out the ones that don\\'t have an English name or a population greater than 100 million. The second `SERVICE` statement is used to obtain the country names using DBpedia property service.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 58, 'completion_tokens': 577, 'total_tokens': 635}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"¿Puedes escribirme una SPARQL query completa (usando Select, filter, service) para obtener el nombre de los países que tienen una población mayor a 100 millones de habitantes?\"\n",
    "\n",
    "prompt_template=f\"<s>[INST] {prompt} [/INST]\"\n",
    "\n",
    "response=llm(prompt=prompt_template, max_tokens=1024, temperature=0.5, top_p=0.95,\n",
    "                  repeat_penalty=1.2, top_k=150,\n",
    "                  echo=True)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dca725b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4080\n",
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "#imprimir version pytorch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6842324-0a81-44fb-a220-905af77601af",
   "metadata": {},
   "source": [
    "# Initializing Dynamic Routes\n",
    "\n",
    "Similar to the `02-dynamic-routes.ipynb` notebook, we will be initializing some dynamic routes that make use of LLMs for function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26db664-9dff-476a-84ef-edd7a8cdf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "from semantic_router import Route\n",
    "from semantic_router.utils.function_call import get_schema\n",
    "\n",
    "\n",
    "def get_time(timezone: str) -> str:\n",
    "    \"\"\"Finds the current time in a specific timezone.\n",
    "\n",
    "    :param timezone: The timezone to find the current time in, should\n",
    "        be a valid timezone from the IANA Time Zone Database like\n",
    "        \"America/New_York\" or \"Europe/London\". Do NOT put the place\n",
    "        name itself like \"rome\", or \"new york\", you must provide\n",
    "        the IANA format.\n",
    "    :type timezone: str\n",
    "    :return: The current time in the specified timezone.\"\"\"\n",
    "    now = datetime.now(ZoneInfo(timezone))\n",
    "    return now.strftime(\"%H:%M\")\n",
    "\n",
    "\n",
    "time_schema = get_schema(get_time)\n",
    "time_schema\n",
    "time = Route(\n",
    "    name=\"get_time\",\n",
    "    utterances=[\n",
    "        \"what is the time in new york city?\",\n",
    "        \"what is the time in london?\",\n",
    "        \"I live in Rome, what time is it?\",\n",
    "    ],\n",
    "    function_schema=time_schema,\n",
    ")\n",
    "\n",
    "politics = Route(\n",
    "    name=\"politics\",\n",
    "    utterances=[\n",
    "        \"isn't politics the best thing ever\",\n",
    "        \"why don't you tell me about your political opinions\",\n",
    "        \"don't you just love the president\" \"don't you just hate the president\",\n",
    "        \"they're going to destroy this country!\",\n",
    "        \"they will save the country!\",\n",
    "    ],\n",
    ")\n",
    "chitchat = Route(\n",
    "    name=\"chitchat\",\n",
    "    utterances=[\n",
    "        \"how's the weather today?\",\n",
    "        \"how are things going?\",\n",
    "        \"lovely weather today\",\n",
    "        \"the weather is horrendous\",\n",
    "        \"let's go to the chippy\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "routes = [politics, chitchat, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac95b0c-c61f-4158-b7d9-0221f7d0b65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_time',\n",
       " 'description': 'Finds the current time in a specific timezone.\\n\\n:param timezone: The timezone to find the current time in, should\\n    be a valid timezone from the IANA Time Zone Database like\\n    \"America/New_York\" or \"Europe/London\". Do NOT put the place\\n    name itself like \"rome\", or \"new york\", you must provide\\n    the IANA format.\\n:type timezone: str\\n:return: The current time in the specified timezone.',\n",
       " 'signature': '(timezone: str) -> str',\n",
       " 'output': \"<class 'str'>\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd15620-92bd-4b77-99f4-c3fe68e9ab62",
   "metadata": {},
   "source": [
    "# Encoders\n",
    "\n",
    "You can use alternative Encoders, however, in this example we want to showcase a fully-local Semantic Router execution, so we are going to use a `HuggingFaceEncoder` with `sentence-transformers/all-MiniLM-L6-v2` (the default) as an embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5253c141-141b-4fda-b07c-a313393902ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from semantic_router.encoders import HuggingFaceEncoder\n",
    "\n",
    "encoder = HuggingFaceEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512fb46e-352b-4740-971e-ad4d047aa03b",
   "metadata": {},
   "source": [
    "# `llama.cpp` LLM\n",
    "\n",
    "From here, we can go ahead and instantiate our `llama-cpp-python` `llama_cpp.Llama` LLM, and then pass it to the `semantic_router.llms.LlamaCppLLM` wrapper class.\n",
    "\n",
    "For `llama_cpp.Llama`, there are a couple of parameters you should pay attention to:\n",
    "\n",
    "- `n_gpu_layers`: how many LLM layers to offload to the GPU (if you want to offload the entire model, pass `-1`, and for CPU execution, pass `0`)\n",
    "- `n_ctx`: context size, limit the number of tokens that can be passed to the LLM (this is bounded by the model's internal maximum context size, in this case for Mistral-7B-Instruct, 8000 tokens)\n",
    "- `verbose`: if `False`, silences output from `llama.cpp`\n",
    "\n",
    "> For other parameter explanation, refer to the `llama-cpp-python` [API Reference](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "772cec0d-7a0c-4c7e-9b7a-4a1864b0a8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./mistral-7b-instruct-v0.2.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "llama_new_context_with_model:        CPU compute buffer size =   156.00 MiB\n",
      "\u001b[32m2024-01-22 00:46:09 INFO semantic_router.utils.logger Initializing RouteLayer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from semantic_router import RouteLayer\n",
    "\n",
    "from llama_cpp import Llama\n",
    "from semantic_router.llms.llamacpp import LlamaCppLLM\n",
    "\n",
    "enable_gpu = True  # offload LLM layers to the GPU (must fit in memory)\n",
    "\n",
    "_llm = Llama(\n",
    "    # model_path=\"./mistral-7b-instruct-v0.2.Q4_0.gguf\",\n",
    "    model_path=\"./mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=-1 if enable_gpu else 0,\n",
    "    n_ctx=2048,\n",
    "    verbose=False,\n",
    ")\n",
    "llm = LlamaCppLLM(name=\"Mistral-7B-v0.2-Instruct\", llm=_llm, max_tokens=None)\n",
    "\n",
    "\n",
    "\n",
    "rl = RouteLayer(encoder=encoder, routes=routes, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8bd1da4-8ff7-4cd3-a5e3-fd79a938cc67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteChoice(name='chitchat', function_call=None, similarity_score=None, trigger=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(\"how's the weather today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6ccbea2-376b-4b28-9b79-d2e9c71e99f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n",
      "\n",
      "\u001b[32m2024-01-22 00:46:28 INFO semantic_router.utils.logger Extracting function input...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='get_time' function_call={'timezone': 'America/New_York'} similarity_score=None trigger=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'18:46'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rl(\"what's the time in New York right now?\")\n",
    "print(out)\n",
    "get_time(**out.function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720f976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n",
      "\n",
      "\u001b[32m2024-01-22 00:32:51 INFO semantic_router.utils.logger Extracting function input...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='get_time' function_call={'timezone': 'Europe/Rome'} similarity_score=None trigger=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'00:33'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rl(\"what's the time in Rome right now?\")\n",
    "print(out)\n",
    "get_time(**out.function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d9dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n",
      "\n",
      "\u001b[32m2024-01-22 00:33:47 INFO semantic_router.utils.logger Extracting function input...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='get_time' function_call={'timezone': 'Asia/Bangkok'} similarity_score=None trigger=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'06:34'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rl(\"what's the time in Bangkok right now?\")\n",
    "print(out)\n",
    "get_time(**out.function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675d12fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n",
      "\n",
      "\u001b[32m2024-01-13 16:41:07 INFO semantic_router.utils.logger Extracting function input...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='get_time' function_call={'timezone': 'Asia/Bangkok'} similarity_score=None trigger=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'23:41'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = rl(\"what's the time in Phuket right now?\")\n",
    "print(out)\n",
    "get_time(**out.function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200f550-f3be-43d7-9b76-6390360f07c8",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df5f53",
   "metadata": {},
   "source": [
    "Once done, if you'd like to delete the downloaded model you can do so with the following:\n",
    "\n",
    "```\n",
    "! rm ./mistral-7b-instruct-v0.2.Q4_0.gguf\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
