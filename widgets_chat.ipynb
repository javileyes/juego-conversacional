{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En construcción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83aedac338c14786a3841097da87f636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "try:\n",
    "  import accelerate\n",
    "except ImportError:\n",
    "  # instala paquetes\n",
    "  %pip install -q accelerate bitsandbytes rich transformers\n",
    "\n",
    "  print(\"instaladas librerías necesarias\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Inicializa la variable model como None al inicio\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "# Función para cargar el modelo si aún no está cargado\n",
    "def load_model():\n",
    "    global model\n",
    "    global tokenizer\n",
    "    if model is None or not hasattr(model, 'num_parameters'):  # Verifica si model está vacío o no parece ser un modelo válido\n",
    "        print(\"Cargando modelo...\")\n",
    "        # modelo sin cuantizar (se queda sin memoria con contexto grande)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"argilla/notus-7b-v1\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"argilla/notus-7b-v1\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)\n",
    "        #torch_dtype=torch.float16  (half precision) or torch.float32 (single precision que sería absurdo porque deepseek coder viene de llama 2 cuyo parámetros son float16)\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "        print(\"Modelo cargado.\")\n",
    "    else:\n",
    "        print(\"Modelo ya estaba cargado.\")\n",
    "\n",
    "\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # codificar \"</s>\" para poder buscarlo en el texto generado\n",
    "fin_generado = tokenizer.encode(\"</s>\", add_special_tokens=False)[0]\n",
    "fin_generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ya estaba cargado.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7f3b01be464c018a8788f642484f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1249d814727d485a9adf0090e2856e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='', description='Input:', layout=Layout(height='100px', width='800px'), placehol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from IPython.display import display, Image, clear_output#, Markdown\n",
    "# from rich.markdown import Markdown\n",
    "from IPython.display import Markdown\n",
    "# import markdown\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "historico = \"\"\n",
    "\n",
    "input_text = \"\"\n",
    "\n",
    "\n",
    "text_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Escribe algo aquí, por ejemplo: /help',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='800px', height='100px'), # Ajusta el tamaño aquí\n",
    "    id='widget-textarea-id'\n",
    ")\n",
    "\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Enviar',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' o ''\n",
    "    tooltip='Enviar',\n",
    "    icon='check' # (FontAwesome names sin el prefijo `fa-`)\n",
    ")\n",
    "\n",
    "def print_wrapped(text, width=120):\n",
    "    words = text.split()\n",
    "    line = ''\n",
    "\n",
    "    for word in words:\n",
    "        if len(line) + len(word) + 1 > width:\n",
    "            print(line)\n",
    "            line = word\n",
    "        else:\n",
    "            if line:\n",
    "                line += ' '\n",
    "            line += word\n",
    "    print(line)\n",
    "\n",
    "def display_response(text, image_path=None):\n",
    "    # text = text.replace(\"�\", \"ú\") #error de este modelo con las \"ú\"\n",
    "    # text = text.replace(\"<|EOT|>\", \"\") #no mostrar este caracter especial\n",
    "    # # display(Markdown(text))  # Para texto\n",
    "    # display_formatted_text(text)  # Para texto con formato\n",
    "    print_wrapped(text)\n",
    "    # Para imágenes (si response contiene una ruta de imagen o URL)\n",
    "    # Mostrar imagen si la ruta está proporcionada\n",
    "    if image_path:\n",
    "        display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "# VENTANA DESLIZANTE\n",
    "def ajustar_contexto(texto, max_longitud=4000, secuencia=\"### Instruction\"):\n",
    "    # Comprobar si la longitud del texto es mayor que el máximo permitido\n",
    "    if len(texto) > max_longitud:\n",
    "        indice_secuencia = 0\n",
    "\n",
    "        while True:\n",
    "            # Buscar la secuencia de ajuste\n",
    "            indice_secuencia = texto.find(secuencia, indice_secuencia + 1)\n",
    "\n",
    "            # Si la secuencia no se encuentra o el texto restante es menor que la longitud máxima\n",
    "            if indice_secuencia == -1 or len(texto) - indice_secuencia <= max_longitud:\n",
    "                break\n",
    "\n",
    "        # Si encontramos una secuencia válida\n",
    "        if indice_secuencia != -1:\n",
    "            return texto[indice_secuencia:]\n",
    "        else:\n",
    "            # Si no se encuentra ninguna secuencia adecuada, tomar los últimos max_longitud caracteres\n",
    "            return texto[-max_longitud:]\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "\n",
    "def eliminar_ultima_pregunta_respuesta(texto, secuencia=\"### Instruction\"):\n",
    "    # Buscar la secuencia de ajuste\n",
    "    indice_secuencia = texto.rfind(secuencia)\n",
    "\n",
    "    # Si la secuencia no se encuentra\n",
    "    if indice_secuencia == -1:\n",
    "        return texto\n",
    "    else:\n",
    "        return texto[:indice_secuencia]\n",
    "\n",
    "\n",
    "def eliminar_ultimas_preguntas_respuestas(texto, n=1, secuencia=\"### Instruction\"):\n",
    "    for i in range(n):\n",
    "        texto = eliminar_ultima_pregunta_respstartswithuesta(texto, secuencia)\n",
    "    return texto\n",
    "\n",
    "# recuperar última pregunta desde ### Instruction hasta ### Response\n",
    "def recuperar_ultima_pregunta(texto, secuencia=\"### Instruction\", fin=\"### Response\"):\n",
    "    # Buscar la secuencia de ajuste\n",
    "    indice_secuencia = texto.rfind(secuencia)\n",
    "    indice_fin = texto.rfind(fin)\n",
    "\n",
    "    # Si la secuencia no se encuentra\n",
    "    if indice_secuencia == -1:\n",
    "        return texto\n",
    "    else:\n",
    "        indice_secuencia += len(secuencia) + 2\n",
    "        return texto[indice_secuencia:indice_fin]\n",
    "\n",
    "\n",
    "def generate_long_chat(contexto, input_text, max_additional_tokens=2000):\n",
    "    global historico\n",
    "    global tokenizer\n",
    "    global model\n",
    "\n",
    "    prompt = f\"<|user|>\\n{input_text}</s>\\n<|assistant|>\\n\"\n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior.\n",
    "\n",
    "    # custom_streamer = CustomTextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    final_prompt = contexto + \"\\n\" + prompt\n",
    "    longitud_prompt_tokens = len(tokenizer.encode(final_prompt))\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    model_inputs = inputs.to(model.device)      # .to(\"cuda\")\n",
    "    outputs = model.generate(**model_inputs,\n",
    "                            streamer=streamer,\n",
    "                            max_new_tokens=max_additional_tokens,\n",
    "                            #  max_length=max_length,\n",
    "                            temperature=0.3,\n",
    "                            top_k=50,\n",
    "                            top_p=0.5,\n",
    "                            pad_token_id = 2,\n",
    "                            # eos_token_id=32021,\n",
    "                            do_sample=True                            \n",
    "                            )\n",
    "\n",
    "    # codificar \"</s>\" para poder buscarlo en el texto generado\n",
    "    # fin_generado = tokenizer.encode(\"</s>\", add_special_tokens=False)[0]\n",
    "\n",
    "    inicio_generado = longitud_prompt_tokens - 1\n",
    "    decoded_output = tokenizer.decode(outputs[0][inicio_generado:], skip_special_tokens=True)\n",
    "\n",
    "    # decoded_output = decoded_output.replace(\"�\", \"ú\") #error de este modelo con las \"ú\"\n",
    "\n",
    "    historico += prompt + decoded_output\n",
    "\n",
    "    text = final_prompt + decoded_output + \"</s>\"\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "load_model()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an helpfull assistant.\n",
    "\"\"\"\n",
    "saludo = \"I am an helpfull assistant. How can I help you?\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "contexto = f\"<|system|>{system_prompt}</s>\\n<|assistant|>\\n{saludo}</s>\\n\"\n",
    "historico = contexto\n",
    "\n",
    "\n",
    "\n",
    "def guardar_historico(historico, nombre_fichero):\n",
    "    with open(nombre_fichero, \"w\", encoding=\"utf-8\") as archivo:\n",
    "        archivo.write(historico)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global contexto, historico, input_text\n",
    "    # Cambiar el estilo del botón a \"procesando\"\n",
    "    button.description = 'Procesando...'\n",
    "    button.button_style = 'warning'  # Color amarillo para indicar procesamiento\n",
    "    button.disabled = True\n",
    "\n",
    "    # Acción a realizar cuando se hace clic en el botón\n",
    "    try:\n",
    "        with output:\n",
    "            # output.clear_output()\n",
    "            print(f\"User: {text_input.value}\")\n",
    "            input_text = text_input.value\n",
    "            text_input.value = \"\"\n",
    "            # input_text = input(\"user: \")\n",
    "            if input_text == \"/new\":\n",
    "                guardar_historico(historico, \"last_session.txt\")\n",
    "                historico = \"\"\n",
    "                contexto = system_prompt\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text == \"/historico\":\n",
    "                clear_output(wait=True)\n",
    "                display_response(historico)\n",
    "            elif input_text == \"/contexto\":\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text.startswith(\"/save\"):\n",
    "                partes = input_text.split(maxsplit=1)\n",
    "                nombre_fichero = partes[1] if len(partes) > 1 else time.strftime(\"%Y-%m-%d_%H-%M\") + \".txt\"\n",
    "                guardar_historico(historico, nombre_fichero)\n",
    "                display_response(f\"Histórico guardado en '{nombre_fichero}'\")\n",
    "            elif input_text == \"/len\":\n",
    "                display_response(\"longitud del contexto en caracteres: \" + str(len(contexto)))\n",
    "            elif input_text.startswith(\"/del\"):\n",
    "                partes = input_text.split()\n",
    "                if len(partes) == 2 and partes[1].isdigit():\n",
    "                    try:\n",
    "                        n = int(partes[1])\n",
    "                    except ValueError:\n",
    "                        n = 1\n",
    "                else:\n",
    "                    n = 1  # Por defecto, eliminar una respuesta\n",
    "                historico+= f\"\\n#############################\\n/del {n}\\n##########################################\\n\"\n",
    "                contexto = eliminar_ultimas_preguntas_respuestas(contexto, n)\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text == \"/repeat\":\n",
    "                historico+= \"\\n#############################\\n/repeat\\n##########################################\\n\"\n",
    "                ultima_pregunta = recuperar_ultima_pregunta(contexto)\n",
    "                contexto = eliminar_ultima_pregunta_respuesta(contexto)\n",
    "                # imprimir ultimas 10 letras del contexto:\n",
    "                print(\"GENERANDO STREAMING... (AUN SIN FORMATO)\")\n",
    "                contexto = generate_long_chat(contexto, input_text=ultima_pregunta, max_additional_tokens=2048)\n",
    "                contexto = ajustar_contexto(contexto)\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text.startswith(\"/help\") or input_text.startswith(\"/?\"):\n",
    "                print(\"\"\"\n",
    "                /new: Nuevo Chat\n",
    "                /historico: mostrar el historico completo (no solo el alcance del contexto)\n",
    "                /contexto: muestra el contexto de la conversación (la zona delimitada que tendrá en cuenta el modelo)\n",
    "                /save [file_name]: guarda el historico conversacional en un fichero.\n",
    "                /len: mostrar la longitud del contexto\n",
    "                /del [n]: eliminar las últimas n respuestas\n",
    "                /repeat: repetir la última respuesta\n",
    "                /clear: borrar el contexto\n",
    "                \"\"\")\n",
    "            elif input_text == \"/clear\":\n",
    "                historico+= \"\\n#############################\\n/clear\\n##########################################\\n\"\n",
    "                contexto = system_prompt\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            else:\n",
    "            # generate response\n",
    "                print(\"GENERANDO STREAMING... (AUN SIN FORMATO)\")\n",
    "                contexto = generate_long_chat(contexto, input_text=input_text, max_additional_tokens=2048)\n",
    "                contexto = ajustar_contexto(contexto)\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "    finally:\n",
    "        # Cambiar el estilo del botón de vuelta a su estado normal\n",
    "        button.description = 'Enviar'\n",
    "        button.button_style = 'success'  # Color verde para indicar listo\n",
    "        button.disabled = False\n",
    "\n",
    "# Crear un output para mostrar los resultados\n",
    "output = widgets.Output()\n",
    "\n",
    "# Asignar la función de callback al evento de clic del botón\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Mostrar el output\n",
    "display(output)\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "display(widgets.HBox([text_input, button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
