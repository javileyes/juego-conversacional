{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# En construcción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/transformers/blob/main/src/transformers/generation/streamers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c125187abdee40eab20e2b36c4dd0a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo cargado.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "try:\n",
    "  import accelerate\n",
    "except ImportError:\n",
    "  # instala paquetes\n",
    "  %pip install -q accelerate bitsandbytes rich transformers\n",
    "\n",
    "  print(\"instaladas librerías necesarias\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Inicializa la variable model como None al inicio\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "\n",
    "# Función para cargar el modelo si aún no está cargado\n",
    "def load_model():\n",
    "    global model\n",
    "    global tokenizer\n",
    "    if model is None or not hasattr(model, 'num_parameters'):  # Verifica si model está vacío o no parece ser un modelo válido\n",
    "        print(\"Cargando modelo...\")\n",
    "        # modelo sin cuantizar (se queda sin memoria con contexto grande)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"argilla/notus-7b-v1\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"argilla/notus-7b-v1\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True)\n",
    "        #torch_dtype=torch.float16  (half precision) or torch.float32 (single precision que sería absurdo porque deepseek coder viene de llama 2 cuyo parámetros son float16)\n",
    "        # tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True)\n",
    "        # model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", device_map='auto', load_in_8bit=True, trust_remote_code=True)\n",
    "        print(\"Modelo cargado.\")\n",
    "    else:\n",
    "        print(\"Modelo ya estaba cargado.\")\n",
    "\n",
    "\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # codificar \"</s>\" para poder buscarlo en el texto generado\n",
    "fin_generado = tokenizer.encode(\"</s>\", add_special_tokens=False)[0]\n",
    "fin_generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo ya estaba cargado.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784306dfda8c42c2800f6b2483e25ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c5041bb34b40058f6fdb5bc470f153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Textarea(value='', description='Input:', layout=Layout(height='100px', width='800px'), placehol…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from IPython.display import display, Image, clear_output#, Markdown\n",
    "# from rich.markdown import Markdown\n",
    "from IPython.display import Markdown\n",
    "# import markdown\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "historico = \"\"\n",
    "\n",
    "input_text = \"\"\n",
    "\n",
    "\n",
    "text_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Escribe algo aquí, por ejemplo: /help',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='800px', height='100px'), # Ajusta el tamaño aquí\n",
    "    id='widget-textarea-id'\n",
    ")\n",
    "\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Enviar',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' o ''\n",
    "    tooltip='Enviar',\n",
    "    icon='check' # (FontAwesome names sin el prefijo `fa-`)\n",
    ")\n",
    "\n",
    "def print_wrapped(text, width=120):\n",
    "    words = text.split()\n",
    "    line = ''\n",
    "\n",
    "    for word in words:\n",
    "        if len(line) + len(word) + 1 > width:\n",
    "            print(line)\n",
    "            line = word\n",
    "        else:\n",
    "            if line:\n",
    "                line += ' '\n",
    "            line += word\n",
    "    print(line)\n",
    "\n",
    "def display_response(text, image_path=None):\n",
    "    # text = text.replace(\"�\", \"ú\") #error de este modelo con las \"ú\"\n",
    "    # text = text.replace(\"<|EOT|>\", \"\") #no mostrar este caracter especial\n",
    "    # # display(Markdown(text))  # Para texto\n",
    "    # display_formatted_text(text)  # Para texto con formato\n",
    "    print_wrapped(text)\n",
    "    # Para imágenes (si response contiene una ruta de imagen o URL)\n",
    "    # Mostrar imagen si la ruta está proporcionada\n",
    "    if image_path:\n",
    "        display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "class AutoLineBreakStreamer(TextStreamer):\n",
    "    \"\"\"\n",
    "    Text streamer that prints the token(s) to stdout as soon as entire words are formed and\n",
    "    automatically inserts line breaks after a specified character limit.\n",
    "\n",
    "    Inherits from TextStreamer.\n",
    "\n",
    "    Parameters:\n",
    "        tokenizer (`AutoTokenizer`): The tokenizer used to decode the tokens.\n",
    "        char_limit (`int`, optional): The character limit after which a line break is inserted.\n",
    "        skip_prompt (`bool`, optional): Whether to skip the prompt to `.generate()`.\n",
    "        decode_kwargs (`dict`, optional): Additional kwargs for the tokenizer's decode method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: \"AutoTokenizer\", char_limit: int = 60, **kwargs):\n",
    "        super().__init__(tokenizer, **kwargs)\n",
    "        self.char_limit = char_limit\n",
    "        self.current_length = 0\n",
    "\n",
    "  \n",
    "    def put(self, value):\n",
    "        \"\"\"\n",
    "        Modifica el método put para insertar un salto de línea cada 80 caracteres.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(value.shape) > 1 and value.shape[0] > 1:\n",
    "            raise ValueError(\"TextStreamer only supports batch size 1\")\n",
    "        elif len(value.shape) > 1:\n",
    "            value = value[0]\n",
    "\n",
    "        if self.skip_prompt and self.next_tokens_are_prompt:\n",
    "            self.next_tokens_are_prompt = False\n",
    "            return\n",
    "\n",
    "        self.token_cache.extend(value.tolist())\n",
    "        text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n",
    "\n",
    "        # Buscar la posición del último salto de línea\n",
    "        last_newline_pos = text.rfind('\\n')\n",
    "        if last_newline_pos == -1:\n",
    "            last_newline_pos = 0\n",
    "\n",
    "        # Verificar si la distancia desde el último salto de línea es de char_limit (50) caracteres o más\n",
    "        if len(text) - last_newline_pos >= self.char_limit:\n",
    "            # print(\"\")\n",
    "            # esperando_espacio = True\n",
    "            # print(f\"log: lentext:{len(text)} - lastsalto:{last_newline_pos} dif:{len(text) - last_newline_pos}\")\n",
    "            next_space = text.rfind(' ', last_newline_pos, len(text))\n",
    "            if next_space != -1:\n",
    "                # Insertar un salto de línea en la siguiente posición de espacio\n",
    "                text_with_break = text[:next_space] + \"\\n\" + text[next_space + 1:]\n",
    "                self.token_cache = self.tokenizer.encode(text_with_break, add_special_tokens=False)\n",
    "                text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n",
    "            # else:\n",
    "            #     # Insertar un salto de línea en la posición actual\n",
    "            #     text_with_break = text + \"\\n\"\n",
    "            #     self.token_cache = self.tokenizer.encode(text_with_break, add_special_tokens=False)\n",
    "            #     text = self.tokenizer.decode(self.token_cache, **self.decode_kwargs)\n",
    "\n",
    "        printable_text = text[self.print_len:]\n",
    "        self.print_len = len(text)\n",
    "\n",
    "        self.on_finalized_text(printable_text)\n",
    "\n",
    "        # self.on_finalized_text(printable_text)\n",
    "        # # Manejar el resto del texto como en TextStreamer original.\n",
    "        if text.endswith(\"\\n\"):\n",
    "            printable_text = text[self.print_len:]\n",
    "            self.token_cache = []\n",
    "            self.print_len = 0\n",
    "        elif len(text) > 0 and self._is_chinese_char(ord(text[-1])):\n",
    "            printable_text = text[self.print_len:]\n",
    "            self.print_len += len(printable_text)\n",
    "        else:\n",
    "            printable_text = text[self.print_len : text.rfind(\" \") + 1]\n",
    "            self.print_len += len(printable_text)   \n",
    "        self.on_finalized_text(printable_text)              \n",
    "\n",
    "# Usage example with the modified AutoLineBreakStreamer\n",
    "# streamer = AutoLineBreakStreamer(tokenizer, skip_prompt=True, decode_kwargs={\"skip_special_tokens\": True})\n",
    "\n",
    "# # Your existing code for generating text with the model\n",
    "# final_prompt = contexto + \"\\n\" + prompt\n",
    "# inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "# model_inputs = inputs.to(model.device)\n",
    "# outputs = model.generate(**model_inputs, streamer=streamer, ...)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# VENTANA DESLIZANTE\n",
    "def ajustar_contexto(texto, max_longitud=4000, secuencia=\"### Instruction\"):\n",
    "    # Comprobar si la longitud del texto es mayor que el máximo permitido\n",
    "    if len(texto) > max_longitud:\n",
    "        indice_secuencia = 0\n",
    "\n",
    "        while True:\n",
    "            # Buscar la secuencia de ajuste\n",
    "            indice_secuencia = texto.find(secuencia, indice_secuencia + 1)\n",
    "\n",
    "            # Si la secuencia no se encuentra o el texto restante es menor que la longitud máxima\n",
    "            if indice_secuencia == -1 or len(texto) - indice_secuencia <= max_longitud:\n",
    "                break\n",
    "\n",
    "        # Si encontramos una secuencia válida\n",
    "        if indice_secuencia != -1:\n",
    "            return texto[indice_secuencia:]\n",
    "        else:\n",
    "            # Si no se encuentra ninguna secuencia adecuada, tomar los últimos max_longitud caracteres\n",
    "            return texto[-max_longitud:]\n",
    "    else:\n",
    "        return texto\n",
    "\n",
    "\n",
    "def eliminar_ultima_pregunta_respuesta(texto, secuencia=\"### Instruction\"):\n",
    "    # Buscar la secuencia de ajuste\n",
    "    indice_secuencia = texto.rfind(secuencia)\n",
    "\n",
    "    # Si la secuencia no se encuentra\n",
    "    if indice_secuencia == -1:\n",
    "        return texto\n",
    "    else:\n",
    "        return texto[:indice_secuencia]\n",
    "\n",
    "\n",
    "def eliminar_ultimas_preguntas_respuestas(texto, n=1, secuencia=\"### Instruction\"):\n",
    "    for i in range(n):\n",
    "        texto = eliminar_ultima_pregunta_respstartswithuesta(texto, secuencia)\n",
    "    return texto\n",
    "\n",
    "# recuperar última pregunta desde ### Instruction hasta ### Response\n",
    "def recuperar_ultima_pregunta(texto, secuencia=\"### Instruction\", fin=\"### Response\"):\n",
    "    # Buscar la secuencia de ajuste\n",
    "    indice_secuencia = texto.rfind(secuencia)\n",
    "    indice_fin = texto.rfind(fin)\n",
    "\n",
    "    # Si la secuencia no se encuentra\n",
    "    if indice_secuencia == -1:\n",
    "        return texto\n",
    "    else:\n",
    "        indice_secuencia += len(secuencia) + 2\n",
    "        return texto[indice_secuencia:indice_fin]\n",
    "\n",
    "\n",
    "def generate_long_chat(contexto, input_text, max_additional_tokens=2000):\n",
    "    global historico\n",
    "    global tokenizer\n",
    "    global model\n",
    "\n",
    "    prompt = f\"<|user|>\\n{input_text}</s>\\n<|assistant|>\\n\"\n",
    "    # streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True) # para streamear el output pero sin repetir el prompt ni el contexto anterior.\n",
    "\n",
    "    # custom_streamer = CustomTextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    streamer = AutoLineBreakStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True, decode_kwargs={\"skip_special_tokens\": True})\n",
    "\n",
    "\n",
    "    final_prompt = contexto + \"\\n\" + prompt\n",
    "    longitud_prompt_tokens = len(tokenizer.encode(final_prompt))\n",
    "\n",
    "    inputs = tokenizer(final_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "    model_inputs = inputs.to(model.device)      # .to(\"cuda\")\n",
    "    outputs = model.generate(**model_inputs,\n",
    "                            streamer=streamer,\n",
    "                            max_new_tokens=max_additional_tokens,\n",
    "                            #  max_length=max_length,\n",
    "                            temperature=0.3,\n",
    "                            top_k=50,\n",
    "                            top_p=0.5,\n",
    "                            pad_token_id = 2,\n",
    "                            # eos_token_id=32021,\n",
    "                            do_sample=True                            \n",
    "                            )\n",
    "\n",
    "    # codificar \"</s>\" para poder buscarlo en el texto generado\n",
    "    # fin_generado = tokenizer.encode(\"</s>\", add_special_tokens=False)[0]\n",
    "\n",
    "    inicio_generado = longitud_prompt_tokens - 1\n",
    "    decoded_output = tokenizer.decode(outputs[0][inicio_generado:], skip_special_tokens=True)\n",
    "\n",
    "    # decoded_output = decoded_output.replace(\"�\", \"ú\") #error de este modelo con las \"ú\"\n",
    "\n",
    "    historico += prompt + decoded_output\n",
    "\n",
    "    text = final_prompt + decoded_output + \"</s>\"\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "load_model()\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an helpfull assistant.\n",
    "\"\"\"\n",
    "saludo = \"I am an helpfull assistant. How can I help you?\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "contexto = f\"<|system|>{system_prompt}</s>\\n<|assistant|>\\n{saludo}</s>\\n\"\n",
    "historico = contexto\n",
    "\n",
    "\n",
    "\n",
    "def guardar_historico(historico, nombre_fichero):\n",
    "    with open(nombre_fichero, \"w\", encoding=\"utf-8\") as archivo:\n",
    "        archivo.write(historico)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    global contexto, historico, input_text\n",
    "    # Cambiar el estilo del botón a \"procesando\"\n",
    "    button.description = 'Procesando...'\n",
    "    button.button_style = 'warning'  # Color amarillo para indicar procesamiento\n",
    "    button.disabled = True\n",
    "\n",
    "    # Acción a realizar cuando se hace clic en el botón\n",
    "    try:\n",
    "        with output:\n",
    "            # output.clear_output()\n",
    "            print(f\"User: {text_input.value}\")\n",
    "            input_text = text_input.value\n",
    "            text_input.value = \"\"\n",
    "            # input_text = input(\"user: \")\n",
    "            if input_text == \"/new\":\n",
    "                guardar_historico(historico, \"last_session.txt\")\n",
    "                historico = \"\"\n",
    "                contexto = system_prompt\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text == \"/historico\":\n",
    "                clear_output(wait=True)\n",
    "                display_response(historico)\n",
    "            elif input_text == \"/contexto\":\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text.startswith(\"/save\"):\n",
    "                partes = input_text.split(maxsplit=1)\n",
    "                nombre_fichero = partes[1] if len(partes) > 1 else time.strftime(\"%Y-%m-%d_%H-%M\") + \".txt\"\n",
    "                guardar_historico(historico, nombre_fichero)\n",
    "                display_response(f\"Histórico guardado en '{nombre_fichero}'\")\n",
    "            elif input_text == \"/len\":\n",
    "                display_response(\"longitud del contexto en caracteres: \" + str(len(contexto)))\n",
    "            elif input_text.startswith(\"/del\"):\n",
    "                partes = input_text.split()\n",
    "                if len(partes) == 2 and partes[1].isdigit():\n",
    "                    try:\n",
    "                        n = int(partes[1])\n",
    "                    except ValueError:\n",
    "                        n = 1\n",
    "                else:\n",
    "                    n = 1  # Por defecto, eliminar una respuesta\n",
    "                historico+= f\"\\n#############################\\n/del {n}\\n##########################################\\n\"\n",
    "                contexto = eliminar_ultimas_preguntas_respuestas(contexto, n)\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text == \"/repeat\":\n",
    "                historico+= \"\\n#############################\\n/repeat\\n##########################################\\n\"\n",
    "                ultima_pregunta = recuperar_ultima_pregunta(contexto)\n",
    "                contexto = eliminar_ultima_pregunta_respuesta(contexto)\n",
    "                # imprimir ultimas 10 letras del contexto:\n",
    "                print(\"GENERANDO STREAMING... (AUN SIN FORMATO)\")\n",
    "                contexto = generate_long_chat(contexto, input_text=ultima_pregunta, max_additional_tokens=2048)\n",
    "                contexto = ajustar_contexto(contexto)\n",
    "                # clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            elif input_text.startswith(\"/help\") or input_text.startswith(\"/?\"):\n",
    "                print(\"\"\"\n",
    "                /new: Nuevo Chat\n",
    "                /historico: mostrar el historico completo (no solo el alcance del contexto)\n",
    "                /contexto: muestra el contexto de la conversación (la zona delimitada que tendrá en cuenta el modelo)\n",
    "                /save [file_name]: guarda el historico conversacional en un fichero.\n",
    "                /len: mostrar la longitud del contexto\n",
    "                /del [n]: eliminar las últimas n respuestas\n",
    "                /repeat: repetir la última respuesta\n",
    "                /clear: borrar el contexto\n",
    "                \"\"\")\n",
    "            elif input_text == \"/clear\":\n",
    "                historico+= \"\\n#############################\\n/clear\\n##########################################\\n\"\n",
    "                contexto = system_prompt\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "            else:\n",
    "            # generate response\n",
    "                print(\"GENERANDO STREAMING... (AUN SIN FORMATO)\")\n",
    "                contexto = generate_long_chat(contexto, input_text=input_text, max_additional_tokens=2048)\n",
    "                contexto = ajustar_contexto(contexto)\n",
    "                clear_output(wait=True)\n",
    "                display_response(contexto)\n",
    "    finally:\n",
    "        # Cambiar el estilo del botón de vuelta a su estado normal\n",
    "        button.description = 'Enviar'\n",
    "        button.button_style = 'success'  # Color verde para indicar listo\n",
    "        button.disabled = False\n",
    "\n",
    "# Crear un output para mostrar los resultados\n",
    "output = widgets.Output()\n",
    "\n",
    "# Asignar la función de callback al evento de clic del botón\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Mostrar el output\n",
    "display(output)\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "display(widgets.HBox([text_input, button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problama: no coincoden cuda version con pytorch cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Sep__8_19:17:24_PDT_2023\n",
      "Cuda compilation tools, release 12.3, V12.3.52\n",
      "Build cuda_12.3.r12.3/compiler.33281558_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/javier/miniconda3\n",
      "handbook                 /home/javier/miniconda3/envs/handbook\n",
      "mistral               *  /home/javier/miniconda3/envs/mistral\n",
      "openvoice2               /home/javier/miniconda3/envs/openvoice2\n",
      "tf                       /home/javier/miniconda3/envs/tf\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (1.13.1)\n",
      "Collecting torch\n",
      "  Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: torchvision in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (0.16.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: torchaudio in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (0.13.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Collecting triton==2.1.0 (from torch)\n",
      "  Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.52)\n",
      "Requirement already satisfied: numpy in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torchvision) (1.22.0)\n",
      "Requirement already satisfied: requests in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from requests->torchvision) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from requests->torchvision) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/javier/miniconda3/envs/mistral/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "Using cached triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: triton, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.16.0\n",
      "    Uninstalling torchvision-0.16.0:\n",
      "      Successfully uninstalled torchvision-0.16.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.13.1\n",
      "    Uninstalling torchaudio-0.13.1:\n",
      "      Successfully uninstalled torchaudio-0.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "openai-whisper 20231106 requires triton==2.0.0, but you have triton 2.1.0 which is incompatible.\n",
      "wavmark 0.0.2 requires torch<2.0, but you have torch 2.1.2 which is incompatible.\n",
      "wavmark 0.0.2 requires torchaudio<2.0, but you have torchaudio 2.1.2 which is incompatible.\n",
      "xformers 0.0.22.post7 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 triton-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch torchvision torchaudio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
